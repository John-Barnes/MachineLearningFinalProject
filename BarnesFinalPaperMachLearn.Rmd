---
title: "Predicting correct and incorrect form in bicep curls, an approach via LDA"
author: "John Barnes"
date: "October 23, 2015"
output: html_document
keep_md: TRUE
---
  
###Introduction/Problem  
  
The Weight Lifting Exercise (WLE) dataset is described in Velloso, E.; Bulling, A.; 
Gellersen, H.; Ugulino, W.; Fuks, H. *Qualitative Activity Recognition of Weight Lifting Exercises.* 
**Proceedings of 4th International Conference in Cooperation with SIGCHI** 
(Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013 and also at the 
HAR website, http://groupware.les.inf.puc-rio.br/har.  
The dataset was created as a challenge for human activity recognition; using 4
physical motion monitors, could an algorithm learn to sort out correct from 
incorrect bicep curls? The 6 participants carefully performed correct bicep curls
and also 4 different standardized kinds of wrong bicep curls, under the guidance
of trainers, so the dataset is intentionally well-sorted and well-separated.
  
###Housekeeping, downloading, data acquisition (cleaning and splitting into 
###training and test components)  
  
This code loads required libraries, downloads the file, reads it into a data table,
cleans and reduces the data table to a more manageable size, and finally splits 
the data table into training and test components.
  
####housekeeping

```{r, load required packages, echo=TRUE}
#ensures required packages in place
suppressMessages(require("data.table")); suppressMessages(require("RGtk2"))
suppressMessages(require("lattice")); suppressMessages(require("plyr"))
suppressMessages(require("grid"));suppressMessages(require("ggplot2")); suppressMessages(require("GGally"))
suppressMessages(require("dplyr")); suppressMessages(require("klaR"));
suppressMessages(require("MASS")); suppressMessages(require("caret"))
```
  
####download and read data into WLEData
  
```{r, echo=TRUE}
DataCatcher<-tempfile()
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              "./DataCatcher",
              "curl")
WLEData<-data.table()
WLEData<-read.table(file="./DataCatcher",
                    header=TRUE,
                    sep=",")
```
  
####cleaning and reduction of dataset
  
Examination of data via View(WLEData)shows what appear to be observations with
summary statistics (which of course would only apply to a group of observations)
about every 23-26 lines. The suspected "summary statistic" columns are otherwise empty. The
usual values in the summary lines are consistent with being the mean of the prior
set of observations.  
  
Kurtosis, skew, and other summary statistic measures seem very
unlikely to be good predictors of classe, so in the cleaning operation that follows,
those columns are deleted (this also removes an enormous number of NAs along
with any need to do anything about them).  
  
I also decided to remove the rows containing summaries.  Although they were only
2.1%  of all observations, since they were just means of prior real observations,
they would reduce the variances slightly. Variance, in turn, might play a part in
the decisions the program will make, so it seemed best to leave them undistorted. 
Rows containing summaries were
removed using the KeepNoSummaries logical vector.
  
```{r removing summary rows and columns, echo=TRUE}
KeepNoSummaries<-is.na(WLEData[,18]) #using first convenient index column
WLEData<-WLEData[KeepNoSummaries,c(1:11,37:49,60:68,84:86,102,113:124,140,151:160)] #col numbers identified by inspection
```

####setting seed and splitting into test and training sets
  
```{r, echo=TRUE}
set.seed(3091)
#split into a training and test set
inTrain <- createDataPartition(y=WLEData$classe,p=0.8, list=FALSE)
trainWLE <- WLEData[inTrain,]
testWLE <- WLEData[-inTrain,]
```
  
###First approach: principal components for each monitor/error combination  

It seemed reasonable that incorrect form with throwing the elbow forward (B) would express primarily on the arm monitor; that incorrect form in not going all the way up (C) or all the way down (D) would express in the forearm and dumbell monitors; and that throwing the hips forward would express primarily in the belt monitor.  

Some preliminary EDA seemed to confirm this argument. First I looked to see whether there were any highly correlated variables (defined by correlation of 80% or higher), and there were quite a few: 

```{r find how many pairs of highly correlated raw variables are in dataset}
#test for highly correlated raw variables, determine how many pairs of them are in data set.
M <- abs(cor(trainWLE[,8:59]))
diag(M) <- 0
PairsHighlyCorrelated<-nrow(which(M > 0.8,arr.ind=T))/2
PairsHighlyCorrelated
```
  
As a preliminary estimate, since there were only 52 independent raw variables, 
then, a PCA should be able to reduce the total number of variables to 52-38=14,
14+19=33 or fewer.  
  
Furthermore, closer examination of the matrix of correlations led to this 
table, which seemed to confirm the hypothesis:

```{r, create table of correlates, echo=TRUE,figure.width=12}
VariableNamesVector<-as.character(colnames(M))

VariableCombos2Inspect<-data.frame()
NameAndCorrelates<-data.frame()

for (i in 1:length(VariableNamesVector)) {
                DimensionName<-VariableNamesVector[i]
                SelectCorrelates<-M[1:52,i]>=.8
                CorrelatesVector<-VariableNamesVector[SelectCorrelates]
                Correlates<-paste0(CorrelatesVector,collapse=",")
                NameAndCorrelates<-cbind(DimensionName,Correlates)
                names(NameAndCorrelates)<-c("Variable","CorrelatesList")
                VariableCombos2Inspect<-rbind(VariableCombos2Inspect,NameAndCorrelates)
}
print(VariableCombos2Inspect)

```
  
As can be seen from the table, there are three groups of high intercorrelation:  
* belt,  
* arm, and  
* forearm+dumbbell   
  
Based on all this, it seemed likely that since in this case correctness (A) was
primarily the absence of error, that finding the principal components which best
discriminated between A and the error in question would be a matter of finding 
the principal components of arm motion that accounted for classe=B, the principal 
components of forearm+dumbbell motion that accounted for classe=C and classe=D, 
and the principal components of belt motion that accounted for classe=E.  
  
So in my initial model I prepared a principal components analysis for each monitor,
and then applied those principal components in a glm model against subsetted data 
to test whether the principal components could distinguish between A and the error
that I would expect that particular monitor to track.  In general accuracies in 
discriminating between A and B based on principal components of arm motion were 
quite poor, around 70%; accuracies in discriminating between A and C or A and D 
based on principal components of forearm and dumbbell motion were about 80%.  
The discrimination based on the belt monitor between A and E was actually pretty 
good, close to 90%.  
  
Because I had been building and testing with a model of linear coefficients, I 
decided to use linear discriminant analysis rather than a tree model as my final 
model. 
  
When I combined the principal components for the different motions and added them 
to the dataset, I was unable to achieve accuracy any better than about 65%. It 
became quite clear that I had trained my model to discriminate A values against 
a limited background, but not to discriminate between any of the other values 
of classe effectively.  

#### Why the monitor-to-error link failed to pay off in a successful model
  
The reason for the failure can be seen in these graphs; for each monitor (arm, 
belt, forearm, dumbbell), I chose the two with the widest normalized variance of
means between classe values. The classes shown as colors. It can be seen 
at once that the geometry of each graph is extremely scrambled; there are areas 
that are mostly one classe or another but to describe the boundaries of those 
areas (which is what linear discriminant analysis does) would require something 
much more complicated than a combination of lines.

```{r, plots of 2 most-varying variables against each other,echo=TRUE, fig.height=10, fig.width=10}
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
plot(x=trainWLE$pitch_belt, y=trainWLE$yaw_belt,col=trainWLE$classe)  
plot(x=trainWLE$roll_arm, y=trainWLE$yaw_arm,col=trainWLE$classe)  
plot(x=trainWLE$roll_forearm, y=trainWLE$pitch_forearm,col=trainWLE$classe)  
plot(x=trainWLE$roll_dumbbell, y=trainWLE$pitch_dumbbell,col=trainWLE$classe)  
```

The mixed quality with respect to the single widest normalized variance of means 
between classe values is even more visible in this group of very simple graphs.
A good discriminating variable would have very little overlap, so that a vertical
line would separate one or more pairs of classes; all of these show enormous 
overlaps across all the classes.  

```{r, plot one-dimensional separability graphs, echo=TRUE,fig.height=10, fig.width=10}
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
plot(x=trainWLE$yaw_belt,y=trainWLE$classe,col=trainWLE$classe)
plot(x=trainWLE$yaw_arm,y=trainWLE$classe,col=trainWLE$classe)
plot(x=trainWLE$pitch_forearm,y=trainWLE$classe,col=trainWLE$classe)
plot(x=trainWLE$pitch_dumbbell,y=trainWLE$classe,col=trainWLE$classe)
```  
  
###Second approach
  
I decided to find out whether the principal components by monitor analysis was 
any better than just doing an lda from the raw results, to get a baseline on 
just how bad my model was.  The raw data baseline was actually much better, 
pushing the accuracy up to about 70.1% in many repeated trials.
  
#### Second approach, tweaked
  
At about that time, I also noticed that on several of the 1-dimensional plots 
(the yaw_belt versus classe graph above is an excellent example) that much of the
data was falling in distinct groups which unfortunately represented all 5 
classes.  The number of such groups was usually 4, with two double-sized groups,
but a few graphs showed 6. This made sense when I realized that the six 
participants were all required to do the exercise correctly and to do all four
errors in the same proportion; what was showing up on some graphs was that, on
some variables recorded by the monitors, their personal styles sometimes differed
more from other personal styles than the errors and correct procedure differed. 
Adding the participant name as a variable raised the accuracy considerably 
for the final model, which I present here.  

### Final model: a simple LDA on 52 monitor outputs plus participant name.
  
Unfortunately, I ran out of time doing all that. So
here's the code for the final model, with some unnecessary output 
suppressed and some notes on the way.
  

```{r, running LDA model on training set repeatedly}
TrControlSpex<-trainControl(method="repeatedCV",
                            number=30,
                              repeats=5,
                              p=.8,
                              savePredictions=TRUE)
ModelFit<-train(trainWLE$classe ~ .,
                method="lda",
                data=trainWLE[,c(2,8:60)],
                trControl=TrControlSpex)
confusionMatrix(trainWLE$classe,predict(ModelFit,trainWLE))
```
The most interesting thing to note here is that the model actually has a
fairly high specificity, though its sensitivity is much lower than desirable.
It's interesting to note also that this model does best at distinguishing 
between classe=A and classe=E; as noted in the graphs above, and confirmed
by checking the LDA loadings on the linear determinants, this seems to be because
the belt monitor is quite sensitive to that distinction.  
  
An overall accuracy of only about 73% is hard to describe as "high," but as can 
be seen here, it is still probably an overestimate:  
  
```{r, Cross-Verify to estimate out-of-sample accuracy}
ModelFit$resample
ResampleAccuracies<-as.numeric(ModelFit$resample[,1])
mean(ResampleAccuracies)
sd(ResampleAccuracies)  

```  
  
Using resampling, the average accuracy is considerably lower (it may not seem like a big difference but in most of these experiments it comes out to between 1.5 and 2 standard deviations lower; standard deviations are quite small on this problem.  
  
 So I am predicting that on my test dataset, accuracy will 
 be somewhere below 73%, but probably
 still above 72%.
   
  And here is the run on my test set:
    
```{r, running LDA model on test set,echo=TRUE}
confusionMatrix(testWLE$classe,predict(ModelFit,testWLE))
```
To my pleasant surprise (or it would be pleasant if I understood it) 
the accuracy on the test data is actually a bit better than the accuracy
on the training data, contrary to expectations. The only thought I 
have is that since the test set is so much smaller, this may just be
an accident brought on by the much higher variance due to the smaller sample size.

